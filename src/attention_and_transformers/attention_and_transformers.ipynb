{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_and_transformers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings"
      ],
      "metadata": {
        "id": "vYX4PY4OSwSk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyYxbxsmSbsz",
        "outputId": "0f39e136-11c2-4d2e-d549-71aa3e378653"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def positional_encoding(max_position, d_model, min_freq=1e-4):\n",
        "    position = np.arange(max_position)\n",
        "    freqs = min_freq**(2*(np.arange(d_model)//2)/d_model)\n",
        "    pos_enc = position.reshape(-1,1)*freqs.reshape(1,-1)\n",
        "    pos_enc[:, ::2] = np.cos(pos_enc[:, ::2])\n",
        "    pos_enc[:, 1::2] = np.sin(pos_enc[:, 1::2])\n",
        "    return pos_enc\n",
        "\n",
        "pos_enc = positional_encoding(10, 5)\n",
        "print(pos_enc.shape)\n",
        "print(pos_enc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5)\n",
            "[[ 1.          0.          1.          0.          1.        ]\n",
            " [ 0.54030231  0.84147098  0.99968454  0.02511622  0.9999998 ]\n",
            " [-0.41614684  0.90929743  0.99873835  0.0502166   0.9999992 ]\n",
            " [-0.9899925   0.14112001  0.99716204  0.07528529  0.99999821]\n",
            " [-0.65364362 -0.7568025   0.99495659  0.10030649  0.99999682]\n",
            " [ 0.28366219 -0.95892427  0.9921234   0.1252644   0.99999502]\n",
            " [ 0.96017029 -0.2794155   0.98866425  0.15014327  0.99999283]\n",
            " [ 0.75390225  0.6569866   0.98458133  0.17492742  0.99999025]\n",
            " [-0.14550003  0.98935825  0.97987722  0.1996012   0.99998726]\n",
            " [-0.91113026  0.41211849  0.97455487  0.22414905  0.99998388]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "B5C804RRS2Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self attention calculations"
      ],
      "metadata": {
        "id": "1c0GAot3TNQv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5QUqgkTs5W3",
        "outputId": "29cfdc3d-c279-41db-e2c3-f173167704ef"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import math\n",
        "from random import choice\n",
        "\n",
        "# x = np.array([choice([0, 1.,2.]) for _ in range(12)])\n",
        "x = np.array([1., 0, 1, 0, 0, 2, 0, 2, 1, 1, 1, 1])\n",
        "x = x.reshape(3,4)\n",
        "\n",
        "query_w = np.array([1., 0, 1., 1, 0, 0, 0, 0, 1, 0, 1, 1])\n",
        "query_w = query_w.reshape(4, 3)\n",
        "\n",
        "\n",
        "key_w = np.array([0, 0, 1., 1, 1, 0, 0, 1, 0, 1, 1, 0])\n",
        "key_w = key_w.reshape(4, 3)\n",
        "\n",
        "value_w = np.array([0, 2, 0, 0, 3, 0, 1, 0, 3, 1, 1, 0])\n",
        "value_w = value_w.reshape(4, 3)\n",
        "\n",
        "Q = x @ query_w\n",
        "K = x @ key_w\n",
        "V = x @ value_w\n",
        "\n",
        "k_d = int(math.sqrt(3))\n",
        "print('k_d:', k_d)\n",
        "\n",
        "attention_score = (Q @ K.transpose())/k_d\n",
        "print(attention_score)\n",
        "\n",
        "attention_score = softmax(attention_score, axis=1) # taking softmax across the rows\n",
        "print(\"attention_score: \")\n",
        "print(attention_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k_d: 1\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]]\n",
            "attention_score: \n",
            "[[6.33789383e-02 4.68310531e-01 4.68310531e-01]\n",
            " [6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
            " [2.95387223e-04 8.80536902e-01 1.19167711e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head attention"
      ],
      "metadata": {
        "id": "LEfnhfgkTHvV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKCt2Dm6s8En",
        "outputId": "a567b270-7930-4aa3-997c-479c116900a8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Assuming the training on the 8 heads of the attention sub-layer is done.\")\n",
        "z0h1=np.random.random((3, 64))\n",
        "z1h2=np.random.random((3, 64))\n",
        "z2h3=np.random.random((3, 64))\n",
        "z3h4=np.random.random((3, 64))\n",
        "z4h5=np.random.random((3, 64))\n",
        "z5h6=np.random.random((3, 64))\n",
        "z6h7=np.random.random((3, 64))\n",
        "z7h8=np.random.random((3, 64))\n",
        "print(\"shape of one head\", z0h1.shape, \"dimension of 8 heads\", 64*8)\n",
        "\n",
        "print(\"Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\")\n",
        "output_attention=np.hstack((z0h1, z1h2, z2h3, z3h4, z4h5, z5h6, z6h7, z7h8))\n",
        "print('final output shape', output_attention.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assuming the training on the 8 heads of the attention sub-layer is done.\n",
            "shape of one head (3, 64) dimension of 8 heads 512\n",
            "Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
            "final output shape (3, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Multi Head attention"
      ],
      "metadata": {
        "id": "t9EPMtEWTXUR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVDQ_h5MtC2X",
        "outputId": "2bd2d113-595d-438a-c1b5-105ba60d0603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import math\n",
        "\n",
        "print('original attention scores (after scaling)')\n",
        "print(attention_score)\n",
        "\n",
        "zeros = np.zeros(attention_score.shape)\n",
        "mx = ma.masked_array(zeros, mask=[0, 1, 1, 0, 0, 1, 0, 0, 0], fill_value=1e+20)\n",
        "\n",
        "print('masked values')\n",
        "print(mx)\n",
        "\n",
        "print('fill the mask')\n",
        "mx = mx.filled(fill_value=float('-inf'))\n",
        "\n",
        "print('add the masking')\n",
        "attention_score1 = attention_score + mx\n",
        "print(attention_score1)\n",
        "\n",
        "attention_score1 = softmax(attention_score1, axis=1) # taking softmax across the rows\n",
        "print(attention_score1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original attention scores (after scaling)\n",
            "[[6.33789383e-02 4.68310531e-01 4.68310531e-01]\n",
            " [6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
            " [2.95387223e-04 8.80536902e-01 1.19167711e-01]]\n",
            "masked values\n",
            "[[0.0 -- --]\n",
            " [0.0 0.0 --]\n",
            " [0.0 0.0 0.0]]\n",
            "fill the mask\n",
            "add the masking\n",
            "[[6.33789383e-02           -inf           -inf]\n",
            " [6.03366485e-06 9.82007865e-01           -inf]\n",
            " [2.95387223e-04 8.80536902e-01 1.19167711e-01]]\n",
            "[[1.         0.         0.        ]\n",
            " [0.27249476 0.72750524 0.        ]\n",
            " [0.22037557 0.53143172 0.24819272]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtpzI9GxtE6C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}