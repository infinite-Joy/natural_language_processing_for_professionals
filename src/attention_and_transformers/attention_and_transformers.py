# -*- coding: utf-8 -*-
"""attention_and_transformers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1go8lmDOSpWsO20zpYiw_pg4gjHS9vd8e
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install transformers
# %pip install bertviz

from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", output_attentions=True)

raw_text = "Hello, my dog is cute"

inputs = tokenizer(raw_text, return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)

import numpy as np
import seaborn as sns
import matplotlib.pylab as plt

enc = tokenizer.encode_plus(
  raw_text,
  add_special_tokens=True,
  return_token_type_ids=False,
  return_attention_mask=False,
  truncation=True
)['input_ids']

text = tokenizer.convert_ids_to_tokens(enc)

output_attention_index = 2
attention_head = 0
layer = 0
batch = 0
attention = outputs.to_tuple()[output_attention_index][layer][batch][attention_head].detach().numpy()
cbar_kws = {"label":"Attention scores", "orientation":"horizontal", 'shrink':1, 'extend':'both', 'extendfrac':0.1, 'ticks':np.arange(0,3), 'drawedges':True }
sns.set(rc={'figure.figsize':(11.7,8.27)})
ax = sns.heatmap(attention, linewidth=0.5, annot=True, xticklabels=text, yticklabels=text, fmt="0.2f", cbar_kws=cbar_kws)
plt.show()

"""## Positional Encodings"""

import numpy as np

def positional_encoding(max_position, d_model, min_freq=1e-4):
    position = np.arange(max_position)
    freqs = min_freq**(2*(np.arange(d_model)//2)/d_model)
    pos_enc = position.reshape(-1,1)*freqs.reshape(1,-1)
    pos_enc[:, ::2] = np.cos(pos_enc[:, ::2])
    pos_enc[:, 1::2] = np.sin(pos_enc[:, 1::2])
    return pos_enc

pos_enc = positional_encoding(10, 5)
print(pos_enc.shape)
print(pos_enc)

"""## Attention

### Self attention calculations
"""

import numpy as np
from scipy.special import softmax
import math
from random import choice

# x = np.array([choice([0, 1.,2.]) for _ in range(12)])
x = np.array([1., 0, 1, 0, 0, 2, 0, 2, 1, 1, 1, 1])
x = x.reshape(3,4)

query_w = np.array([1., 0, 1., 1, 0, 0, 0, 0, 1, 0, 1, 1])
query_w = query_w.reshape(4, 3)


key_w = np.array([0, 0, 1., 1, 1, 0, 0, 1, 0, 1, 1, 0])
key_w = key_w.reshape(4, 3)

value_w = np.array([0, 2, 0, 0, 3, 0, 1, 0, 3, 1, 1, 0])
value_w = value_w.reshape(4, 3)

Q = x @ query_w
K = x @ key_w
V = x @ value_w

k_d = int(math.sqrt(3))
print('k_d:', k_d)

attention_score = (Q @ K.transpose())/k_d
print(attention_score)

attention_score = softmax(attention_score, axis=1) # taking softmax across the rows
print("attention_score: ")
print(attention_score)

"""### Multi Head attention"""

import numpy as np

print("Assuming the training on the 8 heads of the attention sub-layer is done.")
z0h1=np.random.random((3, 64))
z1h2=np.random.random((3, 64))
z2h3=np.random.random((3, 64))
z3h4=np.random.random((3, 64))
z4h5=np.random.random((3, 64))
z5h6=np.random.random((3, 64))
z6h7=np.random.random((3, 64))
z7h8=np.random.random((3, 64))
print("shape of one head", z0h1.shape, "dimension of 8 heads", 64*8)

print("Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model")
output_attention=np.hstack((z0h1, z1h2, z2h3, z3h4, z4h5, z5h6, z6h7, z7h8))
print('final output shape', output_attention.shape)

"""### Masked Multi Head attention"""

import numpy as np
import numpy.ma as ma
import math

print('original attention scores (after scaling)')
print(attention_score)

zeros = np.zeros(attention_score.shape)
mx = ma.masked_array(zeros, mask=[0, 1, 1, 0, 0, 1, 0, 0, 0], fill_value=1e+20)

print('masked values')
print(mx)

print('fill the mask')
mx = mx.filled(fill_value=float('-inf'))

print('add the masking')
attention_score1 = attention_score + mx
print(attention_score1)

attention_score1 = softmax(attention_score1, axis=1) # taking softmax across the rows
print(attention_score1)

