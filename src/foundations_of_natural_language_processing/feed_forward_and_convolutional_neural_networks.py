# -*- coding: utf-8 -*-
"""feed_forward_and_convolutional_neural_networks

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d6DqKmLhWNW-DCrdf-9jEcTebEbITsh0

## Shallow Neural Networks
"""

import tensorflow as tf
from tensorflow.keras import layers


class ShallowNN(tf.keras.Model):
    def __init__(self, vocab_size, emb_dim=50, input_length=128, FFN_units=32,
                nb_classes=2, dropout_rate=0.1,
                training=False, name="shallow"):
        super(ShallowNN, self).__init__(name=name)
        self.embedding = layers.Embedding(vocab_size, emb_dim, input_length=input_length)
        self.pool = layers.GlobalAveragePooling1D()
        self.dense = layers.Dense(FFN_units, activation='relu')
        self.dropout = layers.Dropout(rate=dropout_rate)
        if nb_classes == 2:
            self.last_dense = layers.Dense(units=1, activation="sigmoid")
        else:
            self.last_dense = layers.Dense(units=nb_classes, activation="softmax")

    def call(self, inputs, training):
        print('one sample of input signal', inputs[0])
        embs = self.embedding(inputs)
        print('one embedding layer output', embs[0])
        print('embedding layer', embs.shape)
        x = self.dropout(embs, training=training)
        print('dropout 1', x.shape)
        x = self.pool(x)
        print(x[0])
        print('pooling layer', x.shape)
        x = self.dropout(x, training=training)
        print('dropout 2', x.shape)
        x = self.dense(x)
        print('dense layer', x.shape)
        x = self.dropout(x, training=training)
        print('dropout 3', x.shape)
        output = self.last_dense(x)

        return output

model = ShallowNN(20, emb_dim=2, input_length=3, FFN_units=8, nb_classes=2)
model.build([10, 3])
print(model.summary())

import numpy as np

inputs = np.random.randint(0, high=20, size=(10, 3))
out = model(inputs)
print("*"*10)
print('model output', out)
print('output shape', out.shape)

"""## Convolutional Neural Networks

### convolution example
"""

import numpy as np
from scipy import signal
import matplotlib.pyplot as plt

sig = np.repeat([0., 1., 0.], 100)
win = signal.windows.hann(50)
filtered = signal.convolve(sig, win, mode='same') / sum(win)

fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)
ax_orig.plot(sig)
ax_orig.set_title('Original pulse')
ax_orig.margins(0, 0.1)
ax_win.plot(win)
ax_win.set_title('Filter impulse response')
ax_win.margins(0, 0.1)
ax_filt.plot(filtered)
ax_filt.set_title('Filtered signal')
ax_filt.margins(0, 0.1)
fig.tight_layout()
# plt.savefig('output/graph.png')
plt.show()

"""### tf.keras.layers.Conv1D layer"""

import tensorflow as tf

tf.random.set_seed(42)

## The inputs are 2-length vectors with 4 timesteps, and the batch size is 1
input_shape = (1, 4, 2)
x = tf.random.normal(input_shape)

## there are going to be 5 filters with kernel size is 3
conv_layer = tf.keras.layers.Conv1D(5, 3, activation='relu',input_shape=input_shape[1:])
y = conv_layer(x)
print('input vector', x)
print('output vector', y)
print('weights', conv_layer.weights[0])

print(
  (0.3274685 * -0.0165928  + -0.8426258 * 0.47759396) + 
  (0.3194337 * 0.21096879 + -1.4075519 * -0.17139468) + 
  (-2.3880599 * -0.40390682 + -1.0392479 * 0.11860859)
)

"""### Model Implementation"""

import tensorflow as tf
from tensorflow.keras import layers

tf.random.set_seed(42)


class ConvNN(tf.keras.Model):
    def __init__(self, vocab_size, emb_dim=50, FFN_units=32,
                 nfilters=100, nb_classes=2,
                 dropout_rate=0.2, training=False, name="conv"):
        super(ConvNN, self).__init__(name=name)
        self.embedding = layers.Embedding(vocab_size, emb_dim, trainable=False)
        self.conv = layers.Conv1D(nfilters, 3, activation='relu')
        self.pool = layers.GlobalMaxPooling1D()
        self.dense = layers.Dense(FFN_units, activation='relu')
        self.dropout = layers.Dropout(rate=dropout_rate)
        if nb_classes == 2:
            self.last_dense = layers.Dense(units=1, activation="sigmoid")
        else:
            self.last_dense = layers.Dense(
                units=nb_classes, activation="softmax")

    def call(self, inputs, training):
        print('one sample of input signal', inputs[0])
        embs = self.embedding(inputs)
        print('one embedding layer output', embs[0])
        print('embedding layer', embs.shape)
        x = self.conv(embs)
        print('feature vectors for the different filters', x)
        print('shape after the conv layer', x.shape)
        x = self.pool(x)
        print(x[0])
        print('pooling layer', x.shape)
        x = self.dropout(x, training=training)
        print('dropout 2', x.shape)
        x = self.dense(x)
        print('dense layer', x.shape)
        x = self.dropout(x, training=training)
        print('dropout 3', x.shape)
        output = self.last_dense(x)

        return output

model = ConvNN(20, emb_dim=2, FFN_units=8, nfilters=5, nb_classes=2)
model.build([10, 4])
print(model.summary())

