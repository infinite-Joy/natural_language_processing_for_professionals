# -*- coding: utf-8 -*-
"""sentence_embeddings_in_natural_language_processing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SSD8mNeQVXXgM7AaO9xruke1ju9pwMB2

## install the required modules
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install fasttext
# !pip install -U sentence-transformers
# !pip install -q laserembeddings
# !pip install -q ftfy
# !pip install datasets

"""## common functions

### visualisation and heat map
"""

import seaborn as sns
from sklearn.preprocessing import normalize
import numpy as np

def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

def run_and_plot(messages_, embed, do_normalize=True):
  message_embeddings_ = embed(messages_)
  if do_normalize:
      message_embeddings_ = normalize(message_embeddings_, norm='l2', axis=1)
  plot_similarity(messages_, message_embeddings_, 90)

"""### STS benchmark"""

import pandas
import scipy
import math
import csv
import os
import tensorflow as tf

sts_dataset = tf.keras.utils.get_file(
    fname="Stsbenchmark.tar.gz",
    origin="http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz",
    extract=True)
sts_dev = pandas.read_table(
    os.path.join(os.path.dirname(sts_dataset), "stsbenchmark", "sts-dev.csv"),
    error_bad_lines=False,
    skip_blank_lines=True,
    usecols=[4, 5, 6],
    names=["sim", "sent_1", "sent_2"])
sts_test = pandas.read_table(
    os.path.join(
        os.path.dirname(sts_dataset), "stsbenchmark", "sts-test.csv"),
    error_bad_lines=False,
    quoting=csv.QUOTE_NONE,
    skip_blank_lines=True,
    usecols=[4, 5, 6],
    names=["sim", "sent_1", "sent_2"])
# cleanup some NaN values in sts_dev
sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]

import tensorflow as tf
from sklearn.preprocessing import normalize

def run_sts_benchmark(batch, embed, vectors='use'):
  """Returns the similarity scores"""
  if vectors == 'use':
    sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)
    sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)
  else:
      sts_encode1 = normalize(embed(batch['sent_1'].tolist()), norm='l2', axis=1)
      sts_encode2 = normalize(embed(batch['sent_2'].tolist()), norm='l2', axis=1)
  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)
  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)
  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi
  
  return scores

def compute_pearson(sts_data, embed_fn, vectors='use'):
  dev_scores = sts_data['sim'].tolist()
  scores = []
  for batch in np.array_split(sts_data, 10):
    scores.extend(run_sts_benchmark(batch, embed_fn, vectors))

  pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)
  print('Pearson correlation coefficient = {0}\np-value = {1}'.format(
        pearson_correlation[0], pearson_correlation[1]))

"""## Universal Sentence Encoder"""

import tensorflow as tf
import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-large/5")

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

sent1 = "I am hungry"
sent2 = "I want to eat"

sent1 = embed([sent1])
sent2 = embed([sent2])

print('similarity:', cosine_similarity(sent1, sent2)[0][0])

from absl import logging

word = "Elephant"
sentence = "I am a sentence for which I would like to get its embedding."
paragraph = (
    "Universal Sentence Encoder embeddings also support short paragraphs. "
    "There is no hard limit on how long the paragraph is. Roughly, the longer "
    "the more 'diluted' the embedding will be.")
messages = [word, sentence, paragraph]

# Reduce logging output.
logging.set_verbosity(logging.ERROR)

message_embeddings = embed(messages)

for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
  print("Message: {}".format(messages[i]))
  print("Embedding size: {}".format(len(message_embedding)))
  message_embedding_snippet = ", ".join(
      (str(x) for x in message_embedding[:3]))
  print("Embedding: [{}, ...]\n".format(message_embedding_snippet))

messages = [
    # like this game
    "i really like the ps4.",
    "enjoying it very much.",
    "this game truly is my favorite",

    # ok game
    "ok game - good for the price",
    "ok game prerelease to make money nothing else",
    "a pretty good wwii shooter.",

    # meh!!
    "what a silly game.",
    "this thing is a solid 'meh' for me.",
    "this game is all about action",

    # did not like
    "i couldn't get the hang of the interface.",
    "this game was terrible.",
    'best to pretend this one doesnt exist.'
]

run_and_plot(messages, embed, do_normalize=False)

compute_pearson(sts_dev, embed, vectors='use')

"""## sentence bert embeddings"""

messages = [
    # like this game
    "i really like the ps4.",
    "enjoying it very much.",
    "this game truly is my favorite",

    # ok game
    "ok game - good for the price",
    "ok game prerelease to make money nothing else",
    "a pretty good wwii shooter.",

    # meh!!
    "what a silly game.",
    "this thing is a solid 'meh' for me.",
    "this game is all about action",

    # did not like
    "i couldn't get the hang of the interface.",
    "this game was terrible.",
    'best to pretend this one doesnt exist.'
]

from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')

sentence_embeddings = sbert_model.encode(messages)
print(sentence_embeddings)

run_and_plot(messages, sbert_model.encode)

compute_pearson(sts_dev, sbert_model.encode, vectors='np')

sentence_embeddings[0].reshape(1, -1).shape

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarity(sentence_embeddings[0].reshape(1, -1), sentence_embeddings[1].reshape(1, -1))

"""## average glove based word embeddings"""

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/average_word_embeddings_glove.6B.300d')
embeddings = model.encode(sentences)
print(embeddings)

run_and_plot(messages, model.encode)

compute_pearson(sts_dev, model.encode, vectors='np')

"""## cross lingual sentence embeddings

### laser embeddings
"""

!python -m laserembeddings download-models

from laserembeddings import Laser

laser = Laser()

# if all sentences are in the same language:

embeddings = laser.embed_sentences(
    ['let your neural network be polyglot',
     'use multilingual embeddings!'],
    lang='en')  # lang is only used for tokenization

from typing import List, Union
import numpy as np

def laser_encode(text: Union[str, List[str]], lang='en', normalize=True) -> np.ndarray:
    """
    Encodes a corpus of text using LASER
    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences
    :param lang: 2 digit language code (optional autodetect)
    :return:     embedding matrix
    """
    laser_model = Laser()
    
    if isinstance(text, str):
        sentences = [ text ]
    else:
        sentences = list(text)

    embedding = laser_model.embed_sentences(sentences, lang=lang)
    
    if normalize:
        embedding = embedding / np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)
        
    return embedding

embedding = laser_encode('let your neural network be polyglot')
print(embedding.shape)

from sklearn.metrics.pairwise import cosine_similarity

emb1 = laser_encode('Ein kleines Kind reitet auf einem Pferd.', lang='de')
emb2 = laser_encode('Ein Kind reitet auf einem Pferd.', lang='de')

cosine_similarity(emb1, emb2)

from datasets import load_dataset
import pandas as pd

dataset_de = load_dataset("stsb_multi_mt", name="de", split="dev")
dataset_en = load_dataset("stsb_multi_mt", name="en", split="dev")
dataset_de_df = dataset_de.to_pandas()
dataset_de_df['lang'] = 'de'
dataset_en_df = dataset_en.to_pandas()
dataset_en_df['lang'] = 'en'
dataset_df = pd.concat([dataset_de_df, dataset_en_df])

dataset_df = dataset_df.rename(columns={'sentence1': 'sent_1', 'sentence2': 'sent_2', 'similarity_score': 'sim'})

dataset_df.sample(10)

import seaborn as sns
from sklearn.preprocessing import normalize
import numpy as np

def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

def run_and_plot_laser(messages_, langs, embed=None):
  message_embeddings_ = normalize(embed(messages_, langs), norm='l2', axis=1)
#   print(message_embeddings_.shape)
  plot_similarity(messages_, message_embeddings_, 90)

messages = [
    # english
    "i really like the ps4.",
    "this thing is a solid 'meh' for me.",
    "this game was terrible.",

    # de
    "ich mag die ps4 sehr.",
    "dieses Ding ist ein solides 'meh' f√ºr mich.",
    "Dieses Spiel war schrecklich.",
]

langs = ['en'] * 3 + ['de'] * 3

run_and_plot_laser(messages, langs, laser_encode)

import tensorflow as tf
from sklearn.preprocessing import normalize

def run_sts_benchmark_laser(batch, embed, vectors='use'):
  """Returns the similarity scores"""
  if vectors == 'use':
    sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)
    sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)
  else:
      sts_encode1 = normalize(embed(batch['sent_1'].tolist()), norm='l2', axis=1)
      sts_encode2 = normalize(embed(batch['sent_2'].tolist()), norm='l2', axis=1)
  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)
  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)
  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi
  
  return scores

def compute_pearson_laser(sts_data, embed_fn, vectors='use'):
  dev_scores = sts_data['sim'].tolist()
  scores = []
  for batch in np.array_split(sts_data, 10):
    scores.extend(run_sts_benchmark_laser(batch, embed_fn, vectors))

  pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)
  print('Pearson correlation coefficient = {0}\np-value = {1}'.format(
        pearson_correlation[0], pearson_correlation[1]))
  
compute_pearson_laser(dataset_df, laser_encode, vectors='np') # 0.729461283480811

compute_pearson_laser(sts_dev, laser_encode, vectors='np') # 0.7176004045918506

"""### XLM ROberta model"""

from sentence_transformers import SentenceTransformer

sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')
embeddings = model.encode(sentences)
print(embeddings)

messages = [
    # english
    "i really like the ps4.",
    "this thing is a solid 'meh' for me.",
    "this game was terrible.",

    # de
    "ich mag die ps4 sehr.",
    "dieses Ding ist ein solides 'meh' f√ºr mich.",
    "Dieses Spiel war schrecklich.",
]

run_and_plot(messages, model.encode)

compute_pearson(dataset_df, model.encode, vectors='np')