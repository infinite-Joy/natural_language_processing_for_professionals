{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYqvPSGFmIET"
   },
   "source": [
    "## load the data and run the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUm-FiQUmK65",
    "outputId": "41e1af81-7895-4d70-a665-5008a1ef6a4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import imblearn\n",
    "\n",
    "\n",
    "# map punctuation to space\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) \n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text for better understanding\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', '.')\n",
    "    return text\n",
    "\n",
    "\n",
    "df = pd.read_json(\"/app/Video_Games_5.json\", lines=True)\n",
    "df = df[['reviewText', 'overall']]\n",
    "df = df[df['reviewText'].notnull()]\n",
    "df['reviewText'] = df['reviewText'].apply(text_preprocessing)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HhALbsc9mK3_",
    "outputId": "7ad65ce5-6cae-4f7a-d88b-ba4d6693eeac"
   },
   "outputs": [],
   "source": [
    "df[(df.overall==1) & (df.reviewText.str.contains('go'))].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSpgNKPJBDnX"
   },
   "source": [
    "## cosine similarity of a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BiAsFtfRMt79",
    "outputId": "d8539238-9912-4f5c-e684-bf08e7503de7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = np.array([0.5]).reshape(1, -1)\n",
    "B = np.array([0.5]).reshape(1, -1)\n",
    "\n",
    "print(cosine_similarity(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW_lvn0MBG6r"
   },
   "source": [
    "## one hot encoding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WL_tmzx2M0C-",
    "outputId": "91f21628-1896-4887-da96-9afc33546126"
   },
   "outputs": [],
   "source": [
    "## define input string\n",
    "data = 'the quick brown fox jumped over the lazy dog'\n",
    "consecutive_words = data.split()\n",
    "\n",
    "## construct the dictionary\n",
    "all_words = list(set(consecutive_words))\n",
    "\n",
    "## define a mapping of word to integers\n",
    "word_to_int = dict((w, i) for i, w in enumerate(all_words))\n",
    "int_to_word = dict((i, w) for i, w in enumerate(all_words))\n",
    "\n",
    "## integer encode input data\n",
    "integer_encoded = [word_to_int[w] for w in consecutive_words]\n",
    "\n",
    "## one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "  letter = [0 for _ in range(len(all_words))]\n",
    "  letter[value] = 1\n",
    "  onehot_encoded.append(letter)\n",
    "\n",
    "def argmax(vector):\n",
    "  # since vector is actually a list and its one hot encoding hence the\n",
    "  # maximum value is always 1\n",
    "  return vector.index(1)\n",
    "\n",
    "for vec in onehot_encoded:\n",
    "    print('word={word},\\t vec={vec}'.format(word=int_to_word[argmax(vec)], vec=vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLI8mLwSM21a",
    "outputId": "2a89c21d-6eeb-4dbf-c952-a795c5d994fe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = np.array(onehot_encoded[0]).reshape(1, -1)\n",
    "B = np.array(onehot_encoded[1]).reshape(1, -1)\n",
    "\n",
    "print(cosine_similarity(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGvegvJUP5Ai"
   },
   "source": [
    "## Fasttext Vectors\n",
    "\n",
    "fasttext website: https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors are predownloaded from the website and kept in the image. If you are running this code locally you can download from the link https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3evCWQ2NISu",
    "outputId": "c3b85f76-59b9-49ea-9738-de81b96e0ff0"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## load the model\n",
    "ft = fasttext.load_model('/app/wiki.en.bin')\n",
    "\n",
    "## get the word vectors\n",
    "vector = ft.get_word_vector('vector').reshape(1, -1)\n",
    "matrix = ft.get_word_vector('matrix').reshape(1, -1)\n",
    "\n",
    "## compute and report the similarity\n",
    "print('similarity:', cosine_similarity(vector, matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH49aJv-P-WK"
   },
   "source": [
    "## Glove Embeddings\n",
    "\n",
    "GloVe website: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors are predownloaded from the website and kept in the image. If you are running this code locally you can download from the link http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cefpi5hcN76h"
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "glove2word2vec(\n",
    "    glove_input_file=\"/app/glove.840B.300d.txt\",\n",
    "    word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvN-L0hU_CMp",
    "outputId": "896b6641-b170-4287-e3e8-f2eaac612d39"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## get the glove vector\n",
    "vector = glove_model.wv.get_vector('vector').reshape(1, -1)\n",
    "matrix = glove_model.wv.get_vector('matrix').reshape(1, -1)\n",
    "\n",
    "## compute and report the similarities.\n",
    "print('similarity:', cosine_similarity(vector, matrix))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings_in_natural_language_processing",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
