# -*- coding: utf-8 -*-
"""working_with_small_data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A7hais9MwkCcVCzxKcw1Qer7FEl9d2dJ
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install nlpaug
# !pip install -U imbalanced-learn
# !pip install scikit-learn --upgrade
# !pip install pandas==1.1.5

"""## Dataset"""

import urllib.request as req
from urllib.parse import urlparse
import os
import progressbar
import zipfile
import gzip
import shutil
import json
import pandas as pd
import re
import string
import imblearn

pbar = None


def show_progress(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None

def wget(url):
    a = urlparse(url)
    filename = os.path.basename(a.path)
    if not os.path.isfile(filename):
        req.urlretrieve(url, filename, show_progress)
        print(f'downloaded to {filename}')
    else:
        print(f'file {filename} has already been downloaded')
    return filename

def unzip(filename, directory_to_extract_to=os.getcwd()):
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall(directory_to_extract_to)
        print(f'extraction done {zip_ref.namelist()}')

def gunzip(gzfile, fout):
    with gzip.open(gzfile, 'rb') as f_in:
        with open(fout, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    print(f'{gzfile} extracted to {fout}')


# map punctuation to space
translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) 

def text_preprocessing(df):
    """
    Preprocess the text for better understanding
    
    """
    # trim the whitespace at the edges of the string
    df['reviewText'] = df['reviewText'].str.strip()

    # lowercase the text in the string
    df['reviewText'] = df['reviewText'].str.lower()

    # replace new line with a .

    df['reviewText'] = df['reviewText'].replace('\n', ' . ')

    return df


# filename = wget("https://nlp.stanford.edu/data/glove.6B.zip")
# unzip(filename)
Video_Games_5 = wget('http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz')
df = pd.read_json("./Video_Games_5.json.gz", lines=True, compression='gzip')
df = df.sample(50, random_state=42)
df = df[['reviewText', 'overall']]
df = text_preprocessing(df)
df = df.dropna()
df = df.drop_duplicates()
print(df.shape)

df = df.reset_index(drop=True)
df

"""## Load USE"""

import tensorflow_hub as hub

use = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def get_embs(text):
    if isinstance(text, str):
        text = [text]
    return use(text)

df['embs'] = df['reviewText'].apply(get_embs)

"""## Outlier detection

based on distance from the mean of all the samples
"""

import numpy as np
from scipy.spatial import distance

def detect_outliers(df, label, limit=0.2):
    # get the text and corresponding embeddings
    df = df[df['overall']==label]
    sentences = df.reviewText.values

    limit = int(limit * len(sentences))
    
    embs = df.embs.values
    tensor = np.vstack(embs)

    # get the mean embedding.
    mean = np.mean(tensor, axis=0)

    # Calculate the distance of each instance from the mean
    dist = distance.cdist(tensor, mean.reshape(1, -1), metric='euclidean')
    dist = dist.flatten()

    # Rank by distance in ascending order
    dist_with_index = list(zip(sentences, dist))
    dist_with_index.sort(reverse=True, key=lambda x: x[1])
    
    # Cut off the list, keeping only the top k% as outliers.
    i = 0
    while i < limit:
        sent = dist_with_index[i][0]
        print(sent)
        print('#'*10)
        i += 1

detect_outliers(df, 5)

"""## Data Augmentation"""

import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas
import nlpaug.flow as nafc

from nlpaug.util import Action

df['overall'].value_counts()

df.loc[39, 'reviewText']

for _, row in df[df.overall==3].iterrows():
    print(row['reviewText'])
    print("#"*10)

df = df[df['overall']!=2]
print(df.shape)

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
type_one_hot = encoder.fit_transform(df.overall.to_numpy().reshape(-1, 1))

from sklearn.model_selection import train_test_split

df_train, df_test, y_train, y_test = train_test_split(
    df, type_one_hot, stratify=df['overall'], test_size=.2, random_state=42)
emb_train = np.vstack(df_train.embs)
emb_test = np.vstack(df_test.embs)
print(df_train.shape, df_test.shape, y_train.shape, y_test.shape, emb_train.shape, emb_test.shape)

from imblearn.over_sampling import RandomOverSampler
from collections import Counter

ros = RandomOverSampler(sampling_strategy='minority', random_state=0)
df_train_resampled, y_resampled = RandomOverSampler().fit_resample(df_train[['reviewText', 'overall']], y_train)
df_train_resampled = pd.DataFrame(df_train_resampled, columns=['reviewText', 'overall'])
df_train_resampled['overall'].value_counts()

df_train_resampled[df_train_resampled.overall==1].head()

import tensorflow as tf
from tqdm import tqdm
import numpy as np

import tensorflow_hub as hub

use = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def get_embs(text):
    if isinstance(text, str):
        text = [text]
    return use(text)

X_train = []
for r in tqdm(df_train_resampled.reviewText):
    emb = get_embs(r)
    review_emb = tf.reshape(emb, [-1]).numpy()
    X_train.append(review_emb)
X_train = np.array(X_train)

X_test = []
for r in tqdm(df_test.reviewText):
    emb = get_embs(r)
    review_emb = tf.reshape(emb, [-1]).numpy()
    X_test.append(review_emb)
X_test = np.array(X_test)

"""### Model without augmentation

Since the data is very less, there is no point in splitting in train and validation
"""

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

def build_model(X_train, y_train, epochs=10):
    model = Sequential()
    model.add(Dense(units=256,input_shape=(X_train.shape[1],),activation='relu'))
    # model.add(Dropout(rate=0.5))
    model.add(Dense(units=128,activation='relu'))
    # model.add(Dropout(rate=0.5))
    model.add(Dense(4, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])

    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=16,
        verbose=1,
        shuffle=True
    )

    return history, model


history, random_aug_model = build_model(X_train, y_resampled, 3)

import matplotlib.pyplot as plt

def plot_history(history):
    ## summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train'], loc='upper left')
    plt.show()

    ## summarize history for loss
    plt.plot(history.history['loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train'], loc='upper left')
    plt.show()

plot_history(history)

loss, acc = random_aug_model.evaluate(X_test, y_test)
print('loss:', loss, 'accuracy:', acc)

!pip install -U imbalanced-learn

from sklearn.metrics import accuracy_score, matthews_corrcoef
from imblearn.metrics import classification_report_imbalanced

y_pred = np.argmax(random_aug_model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

print('accuracy:', accuracy_score(y_true, y_pred))
print('matthews_corrcoef:', matthews_corrcoef(y_true, y_pred))
print('classification_report:\n', classification_report_imbalanced(y_true, y_pred))

"""### Model performance after augmentation

check the label distribution before augmenting. label 5 is the largest label
"""

df_train.overall.value_counts()

"""code for nlp augmentation"""

from random import choice, random

def remove_random_space(text):
    indices = [i for i, x in enumerate(text) if x == " "]
    random_space_index = choice(indices)
    return "".join([x for i, x in enumerate(text) if i!=random_space_index])

def delete_char_randomly(text):
    random_char_indx = int(random() * len(text))
    # print(random_char_indx)
    return "".join([x for i, x in enumerate(text) if i!=random_char_indx])

def keyboard_augmenter(text):
    aug = nac.KeyboardAug()
    augmented_text = aug.augment(text)
    return augmented_text

def augment_label(df_train, label, n_out):
    sentences = df_train[df_train.overall==label].reviewText.to_list()

    aug_sentences = []
    factor = round(n_out/(4*len(sentences)))
    sentences = sentences * factor
    for sent in sentences:
        aug_sentences.append(sent)
        aug_sentences.append(remove_random_space(sent))
        aug_sentences.append(keyboard_augmenter(sent))
        aug_sentences.append(delete_char_randomly(sent))

    aug_df = pd.DataFrame(aug_sentences, columns=['reviewText'])
    aug_df['overall'] = label

    return aug_df


# get the augmented values
df_train2 = df_train[df_train.overall==5]
print(df_train2.shape)
aug_df4 = augment_label(df_train, 4, 28)
aug_df3 = augment_label(df_train, 3, 28)
aug_df1 = augment_label(df_train, 1, 28)
df_train2 = pd.concat([df_train2, aug_df4, aug_df3, aug_df1], ignore_index=True)

# shuffle the dataframe so that same labels are not together
df_train2 = df_train2.sample(frac=1, random_state=42)
print(df_train2.shape)
print(df_train2.overall.value_counts())

"""recompute the X_train and y_train as the training distributions are now different."""

y_train = encoder.transform(df_train2.overall.to_numpy().reshape(-1, 1))

X_train = []
for r in tqdm(df_train2.reviewText):
    emb = get_embs(r)
    review_emb = tf.reshape(emb, [-1]).numpy()
    X_train.append(review_emb)
X_train = np.array(X_train)
print(X_train.shape)

"""train the model using 3 epochs"""

history, nlp_aug_model = build_model(X_train, y_train, 3)
plot_history(history)

loss, acc = nlp_aug_model.evaluate(X_test, y_test)
print('loss:', loss, 'accuracy:', acc)

"""the simple augmentation model has mostly overfitted, but there is some other values as part of the nlp augmented model."""

print('simple oversampling outputs:', np.argmax(random_aug_model.predict(X_test), axis=1))
print('nlp model outputs:', np.argmax(nlp_aug_model.predict(X_test), axis=1))
print('actual outputs:', np.argmax(y_test, axis=1))

from sklearn.metrics import accuracy_score, matthews_corrcoef
from imblearn.metrics import classification_report_imbalanced

y_pred = np.argmax(nlp_aug_model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

print('accuracy:', accuracy_score(y_true, y_pred))
print('matthews_corrcoef:', matthews_corrcoef(y_true, y_pred))
print('classification_report:\n', classification_report_imbalanced(y_true, y_pred))

