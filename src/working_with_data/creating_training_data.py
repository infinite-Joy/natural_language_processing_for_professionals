# -*- coding: utf-8 -*-
"""creating_training_data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QwHpgZyR8mfs4UlzbD241-HjFy9xgOHa
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Load the dataset"""

import urllib.request as req
from urllib.parse import urlparse
import os
import progressbar
import zipfile
import gzip
import shutil
import json
import pandas as pd
import re
import string
import imblearn

pbar = None


def show_progress(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None

def wget(url):
    a = urlparse(url)
    filename = os.path.basename(a.path)
    if not os.path.isfile(filename):
        req.urlretrieve(url, filename, show_progress)
        print(f'downloaded to {filename}')
    else:
        print(f'file {filename} has already been downloaded')
    return filename

def unzip(filename, directory_to_extract_to=os.getcwd()):
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall(directory_to_extract_to)
        print(f'extraction done {zip_ref.namelist()}')

def gunzip(gzfile, fout):
    with gzip.open(gzfile, 'rb') as f_in:
        with open(fout, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    print(f'{gzfile} extracted to {fout}')


# map punctuation to space
translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) 

def text_preprocessing(df):
    """
    Preprocess the text for better understanding
    
    """
    # trim the whitespace at the edges of the string
    df['reviewText'] = df['reviewText'].str.strip()

    # lowercase the text in the string
    df['reviewText'] = df['reviewText'].str.lower()

    # replace new line with a .

    df['reviewText'] = df['reviewText'].replace('\n', ' . ')

    return df


filename = wget("https://nlp.stanford.edu/data/glove.6B.zip")
unzip(filename)
Video_Games_5 = wget('http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz')
df = pd.read_json("./Video_Games_5.json.gz", lines=True, compression='gzip')
df = df[['reviewText', 'overall']]
df = text_preprocessing(df)
df = df.dropna()
df = df.drop_duplicates()
print(df.shape)

"""## Taking a sample

Just taking a sample where the where the number of sentences is only 1. That is just a small sample of the total
"""

print('all sentences', len(df))
total_df = df
df = df[df.reviewText.str.split('.').str.len()==1]
df = df[df.reviewText.str.split('!').str.len()==1]
df = df[df.reviewText.str.split('\n').str.len()==1]
print('only single sentences', len(df))
df = df.drop_duplicates()
print('after removing the duplicates', len(df))

"""## Convert to vectors

In order to process the textual data using clustering algorithms, they need to be converted to vectors. Universal Sentence Encoder is a great model to get the vectors as the models are finetuned using STS benchmarking
"""

import tensorflow as tf
import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-large/5")

def get_embeddings(text):
    if not isinstance(text, list):
        text = [text]
    return embed(text)

embs = get_embeddings('The quick brown fox jumps over the lazy dog.')
print(embs.shape)

from tqdm import tqdm, trange
import numpy as np
import math

def run_get_embeddings(df, chunk_size=100):
    sentences = df.reviewText.values.tolist()
    total_size = len(df)
    n_chunks = math.ceil(total_size/chunk_size)
    embeddings_list = []

    for start in trange(n_chunks):
        start = start * chunk_size
        stop = start + chunk_size
        embeddings = embed(sentences[start:stop])
        embeddings_list.append(embeddings)
        # print(embeddings.shape)

    # flatten
    embeddings_list0 = embeddings_list[:-1]
    embeddings_list1 = embeddings_list[-1]
    embeddings_list0 = np.array(embeddings_list0)
    embeddings_list0 = embeddings_list0.reshape(-1, embeddings_list0.shape[-1])
    embeddings_list = np.concatenate((embeddings_list0, embeddings_list1), axis=0)
    return embeddings_list


df['embs'] = list(run_get_embeddings(df))

import numpy as np

embs = df.embs
embs = np.vstack(embs)
print(embs.shape)

np.savez('/content/drive/MyDrive/embs.npz', embs=embs)

import numpy as np

npz = np.load('/content/drive/MyDrive/embs.npz')
embs = npz['embs']
print(embs.shape)

df['embs'] = list(embs)

"""## Dimensionality Reduction"""

from sklearn.decomposition import PCA

# using PCA to reduce the dimensionality
pca = PCA(n_components=10, whiten=False, random_state=42)
abstracts_pca = pca.fit_transform(embs)
print(abstracts_pca.shape)

df['pca'] = list(abstracts_pca)

"""## Kmeans"""

!pip install -U yellowbrick

from sklearn.cluster import KMeans
from yellowbrick.cluster.elbow import kelbow_visualizer

# Use the quick method and immediately show the figure
kelbow_visualizer(KMeans(random_state=42), abstracts_pca, k=(2,100))

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
# %matplotlib inline

def elbow_plot(range_, data, figsize=(15,8)):
    '''
    elbow plot function to help find the right amount of clusters for a dataset
    '''
    inertia_list = []
    range_ = list(range_)
    for n in tqdm(range_):
        kmeans = KMeans(n_clusters=n, random_state=42)
        kmeans.fit(data)
        inertia_list.append(kmeans.inertia_)
        
    # plotting
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    sns.lineplot(y=inertia_list, x=range_, ax=ax)
    ax.set_xlabel("Cluster")
    ax.set_ylabel("Inertia")
    ax.set_xticks(list(range_))
    fig.show()
    fig.savefig("elbow_plot.png")

elbow_plot(range(2, 100, 10), abstracts_pca)

from sklearn.preprocessing import StandardScaler as SS
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
import numpy as np

# plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
from yellowbrick.cluster import SilhouetteVisualizer

def silhouettePlot(range_, data, figsize=(15,8)):
    '''
    we will use this function to plot a silhouette plot that helps us to evaluate the cohesion in clusters (k-means only)
    '''
    half_length = int(len(range_)/2)
    range_list = list(range_)
    fig, ax = plt.subplots(half_length, 2, figsize=figsize)
    for _ in range_:
        kmeans = KMeans(n_clusters=_, random_state=42)
        q, mod = divmod(_ - range_list[0], 2)
        sv = SilhouetteVisualizer(kmeans, colors="yellowbrick", ax=ax[q][mod])
        ax[q][mod].set_title("Silhouette Plot with n={} Cluster".format(_))
        sv.fit(data)
    fig.tight_layout()
    fig.show()
    fig.savefig("silhouette_plot.png")

silhouettePlot(range(3,9), abstracts_pca)



from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=20, random_state=42)
kmeans_labels = kmeans.fit_predict(abstracts_pca)
df["cluster"] = kmeans_labels

"""## the clusters"""

df.cluster.value_counts()

def list_labels(df, cluster_labels, column, sample_limit=10):
    for label in list(np.unique(cluster_labels))[1:]:
        print(f'printing for label={label}')
        label_df = df[df[column]==label][['reviewText']]
        sample_count = len(label_df)
        if sample_count > sample_limit:
            sample_count = sample_limit
        for _, row in label_df.sample(sample_count).iterrows():
            print(row['reviewText'])
        print('%'*10)

list_labels(df, kmeans_labels, 'cluster')

"""## DBSCAN"""

from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt

def findOptimalEps(n_neighbors, data):
    '''
    function to find optimal eps distance when using DBSCAN
    reference: https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc
    '''
    neigh = NearestNeighbors(n_neighbors=n_neighbors)
    nbrs = neigh.fit(data)
    distances, indices = nbrs.kneighbors(data)
    distances = np.sort(distances, axis=0)
    distances = distances[:,1]
    plt.plot(distances)

findOptimalEps(5, abstracts_pca)

from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.15, min_samples=5, metric="cosine")
dbscan_labels = dbscan.fit_predict(embs)
df["dbscan_labels"] = dbscan_labels

df.dbscan_labels.value_counts()

list_labels(df, dbscan_labels, 'dbscan_labels')

"""## select the clusters

Based on the data, since we do not have the labels predefined to us we can consider 3 labels which are "thumps up", "neutral", "thumbs down". There will also be an UNK label that will just act as a dump for sentences where the model is not very sure.

Went through the data and added the clusters below for the labels
"""

import pandas as pd

thumbs_up_clusters = [58, 53, 51, 46, 43, 40, 39, 38, 37, 36, 35, 31, 30, 28, 27, 25, 22, 21, 18, 15, 8, 7]
thumbs_down_clusters = [55, 45, 44, 42, 34, 23, 20, ]
neutral_clusters = [56, 47, 52, 50, 49, 48, 33, 32, 24, ]
unknown_clusters = [6, 5, 3, 1]

def get_labels(dic, clusters, label):
    for cluster in clusters:
        dic[cluster] = label

cluster_label_defined_label_mapping = {}
get_labels(cluster_label_defined_label_mapping, thumbs_up_clusters, 'up')
get_labels(cluster_label_defined_label_mapping, thumbs_down_clusters, 'down')
get_labels(cluster_label_defined_label_mapping, neutral_clusters, 'neutral')
get_labels(cluster_label_defined_label_mapping, unknown_clusters, 'UNK')
# print(cluster_label_defined_label_mapping)

def create_training(df):
    training_df = pd.DataFrame(columns=['reviewText', 'label'])
    for cluster_id in list(np.unique(dbscan_labels)):
        cluster_label = cluster_label_defined_label_mapping.get(cluster_id)
        if cluster_label:
            cluster_df = df[df.dbscan_labels==cluster_id][['reviewText']]
            cluster_df['label'] = cluster_label
            training_df = training_df.append(cluster_df, ignore_index=True)
    return training_df

training_df = create_training(df)
training_df.sample(5)

training_df.sample(10)

training_df.label.value_counts()

"""## feed into the model and train"""

from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(training_df, test_size=0.3, stratify=training_df.label, random_state=42)

X_train = df_train['reviewText']
y_train = df_train['label']

X_test = df_test['reviewText']
y_test = df_test['label']

print(len(df_train), len(df_test))

from imblearn.over_sampling import RandomOverSampler
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, matthews_corrcoef
from imblearn.metrics import classification_report_imbalanced

vectorizer = CountVectorizer(stop_words=['elder scroll','halo'])
X_train_title_vec = vectorizer.fit_transform(X_train)
X_test_title_vec = vectorizer.transform(X_test)

X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train_title_vec, y_train)
print(sorted(Counter(y_resampled).items()))

# for alpha in np.linspace(0,2,20)[1:]:
model = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test_title_vec)

print('accuracy:', accuracy_score(df_test['label'], y_pred))
print('matthews_corrcoef:', matthews_corrcoef(df_test['label'], y_pred))
print('classification_report:\n', classification_report_imbalanced(df_test['label'], y_pred))

"""# inference and evaluate"""

df.sample(10)

prediction_vec = vectorizer.transform(df.reviewText)
y_pred = model.predict(prediction_vec)

df['prediction'] = y_pred

df.prediction.value_counts()

df[df.prediction=='up'].sample(10, random_state=42)

df[df.prediction=='neutral'].sample(10, random_state=42)

df[(df.prediction=='down')].sample(10, random_state=42)

df[(df.prediction=='UNK')].sample(10, random_state=42)

""" Improving the labels. as part of the business process, based on user action, you can infer if the text classification was correct or not and then feed that back to the system . In this way you have a system that continually learns by itself."""