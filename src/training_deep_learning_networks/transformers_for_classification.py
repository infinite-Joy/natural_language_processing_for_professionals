# -*- coding: utf-8 -*-
"""transformers_for_classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VHBESI9j7H5IQpWbUlaCT1UDzL8ezPRl
"""

from google.colab import drive
drive.mount('/content/drive')

!ls -ltr /content/drive/MyDrive/educative_natural_language_processing_for_professionals/

"""# Using transformers for classification"""

!pip freeze | grep torch

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # !pip install torch
# !pip install transformers==4.12.5
# !pip install datasets==1.15.1

!pip freeze | grep torch
!pip freeze | grep transformers
!pip freeze | grep datasets

"""### prepare the data"""

import urllib.request as req
from urllib.parse import urlparse
import os
import progressbar
import zipfile
import gzip
import shutil
import json
import pandas as pd
import re
import string

pbar = None


def show_progress(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None

def wget(url):
    a = urlparse(url)
    filename = os.path.basename(a.path)
    if not os.path.isfile(filename):
        req.urlretrieve(url, filename, show_progress)
        print(f'downloaded to {filename}')
    else:
        print(f'file {filename} has already been downloaded')
    return filename

def unzip(filename, directory_to_extract_to=os.getcwd()):
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall(directory_to_extract_to)
        print(f'extraction done {zip_ref.namelist()}')

# map punctuation to space
translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) 

def text_preprocessing(text):
    """
    Preprocess the text for better understanding
    
    """
    if isinstance(text, str):
        text = text.strip()
        text = text.lower()
    return text


Video_Games_5 = wget('http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz')
df = pd.read_json("./Video_Games_5.json.gz", lines=True, compression='gzip')
df = df[['reviewText', 'overall']]
df['reviewText'] = df.reviewText.apply(text_preprocessing)
df = df.dropna()
df = df.drop_duplicates()
print(df.shape)

df.to_csv('/content/drive/MyDrive/educative_natural_language_processing_for_professionals/data.csv', index=False)

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/educative_natural_language_processing_for_professionals/data.csv')

print(df.sample(5, random_state=42))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

ax = df[df['reviewText'].str.len()<128]['reviewText'].str.split()\
    .map(len)\
    .hist()
ax.set_xlabel("number of words in the message")
ax.set_ylabel("messages")
plt.show()

len(df[df['reviewText'].str.len()<128])/len(df)

"""### tokenisation"""

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
max_length_test = 128

test_sentence = 'Test tokenization sentence. Followed by another sentence'

# add special tokens
test_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'
tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)
print('tokenized', tokenized)

from pprint import pprint

# convert tokens to ids in WordPiece
input_ids = tokenizer.convert_tokens_to_ids(tokenized)
  
# precalculation of pad length, so that we can reuse it later on
padding_length = max_length_test - len(input_ids)

# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length
input_ids = input_ids + ([0] * padding_length)

# attention should focus just on sequence with non padded tokens
attention_mask = [1] * len(input_ids)

# do not focus attention on padded tokens
attention_mask = attention_mask + ([0] * padding_length)

# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence
token_type_ids = [0] * max_length_test

bert_input = {
    "token_ids": input_ids,
    "token_type_ids": token_type_ids,
    "attention_mask": attention_mask
}

print(bert_input)

bert_input = tokenizer.encode_plus(
    test_sentence,                      
    add_special_tokens = True, # add [CLS], [SEP]
    max_length = max_length_test, # max length of the text that can go to BERT
    padding = True, # add [PAD] tokens
    truncation=True,
    return_attention_mask = True, # add attention mask to not focus on pad tokens
)

print('encoded', bert_input)



import torch

labels = list(df.overall.unique())

def tok(example):
  encodings = tokenizer.encode_plus(
        example['reviewText'],                      
        add_special_tokens = True, # add [CLS], [SEP]
        max_length = max_length_test, # max length of the text that can go to BERT
        padding = True, # add [PAD] tokens
        return_attention_mask = True, # add attention mask to not focus on pad tokens
    )
  return encodings

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, random_state=42, test_size=0.3, stratify=df.overall)
print(train_df.shape, test_df.shape)

train_df, unlabeled_df = train_test_split(train_df, random_state=42, test_size=0.9, stratify=train_df.overall)
print(train_df.shape, unlabeled_df.shape, test_df.shape)

# train_df.to_csv("/content/drive/MyDrive/train_df.csv", index=False)
# train_df.to_csv("./train_df.csv", index=False)

# import pandas as pd

# train_df = pd.read_csv('./train_df.csv')

train_df.head()

import pandas as pd
from tqdm import tqdm

def sentences_and_labels(df):
    # training data
    sentences = train_df.reviewText.values
    sentences = ["[CLS] " + sentence + " [SEP]" for sentence in tqdm(sentences)]

    # labels
    number_labels = len(train_df.overall.unique())
    labels = train_df.overall.values
    labels = pd.Series(labels)
    labels = pd.get_dummies(labels)
    labels = labels.values
    labels = labels.astype(float)

    return sentences, labels, number_labels

"""## Prepare data for training"""

sentences, labels, number_labels = sentences_and_labels(train_df)

train_df.overall.unique()

print(len(sentences), len(labels))
labels[:2]

model_type = 'bert-base-uncased'

from transformers import BertTokenizer
from tqdm import tqdm

tokenizer = BertTokenizer.from_pretrained(model_type, do_lower_case=True)
tokenised_text = [tokenizer.tokenize(sent) for sent in tqdm(sentences)]
print("Tokenize the first sentence")
print(tokenised_text[0])

"""max length is 128 because it is memory failures in the training data"""

MAX_LEN = 128

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm

input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenised_text)]
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

attention_masks = []
for seq in tqdm(input_ids):
    seq_mask = [float(i>0) for i in seq]
    attention_masks.append(seq_mask)

from sklearn.model_selection import train_test_split

train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1, stratify=labels)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)

import torch

train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)
train_labels = torch.tensor(train_labels)

validation_labels = torch.tensor(validation_labels)
train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

train_labels[:2]

"""#### the batches should be balanced since the data is imbalanced."""

train_df.overall.value_counts()

import numpy as np
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler

batch_size = 32

def create_dataloader(inputs, masks, labels, batch_size=batch_size, weights=None):
    data = TensorDataset(inputs, masks, labels)
    if weights is not None:
        sampler = WeightedRandomSampler(weights, len(weights), replacement=True)
    else:
        sampler = RandomSampler(data)
    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, num_workers=2)
    return dataloader

counts, num_labels = train_labels.shape
class_weights = np.array([int(sum(train_labels[:, x]).detach().cpu().numpy()) for x in range(num_labels)])
# total = sum(class_weights)
# class_weights = [x/total for x in class_weights]
class_weights = 1. / class_weights
print(class_weights)
labels_unique = list(range(num_labels))

training_weights = [class_weights[int(np.argmax(e))] for e in train_labels]
print(sum(training_weights))
train_dataloader = create_dataloader(train_inputs, train_masks, train_labels, weights=training_weights)

# will not use weighted sampler for the validation weights
validation_dataloader = create_dataloader(validation_inputs, validation_masks, validation_labels)

"""### bert sequence model"""

import transformers
from transformers import BertModel, BertConfig
from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup

configuration = BertConfig()
model = BertModel(configuration)
configuration = model.config
print(configuration)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = BertForSequenceClassification.from_pretrained(model_type, num_labels=number_labels)
if device.type != 'cpu':
    model.cuda()

"""### overview of the different model layers"""

# Get all of the model's parameters as a list of tuples.
params = list(model.named_parameters())

print('The BERT model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer ====\n')

for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

"""### Differential Learning rates for Transfer Learning"""

param_optimizer = list(model.named_parameters())
no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    {
        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        "weight_decay_rate": 0.1
        },
    {
        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        "weight_decay_rate": 0.0
        },
]


epochs = 10

optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)
total_steps = len(train_dataloader) * epochs
num_warmup_steps = int(.1 * total_steps)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)

"""### model training

* set the number of epochs to increase or decrease the number of epochs
* printing_step controls when the training loss is printed. This is made to print 5 times per epoch here
* models are saved at the end of each epoch
"""

import random
from tqdm import tqdm, trange
import numpy as np
from datetime import datetime
from sklearn.metrics import accuracy_score, matthews_corrcoef

# Set the seed value all over the place to make this reproducible.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

count = 0
t0 = datetime.utcnow()

printing_step = max(int(len(train_dataloader) / 5), 1)

t =  []
train_loss_set = []
for epoch in range(epochs):
    # training phase
    model.train()
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    for step, batch in enumerate(tqdm(train_dataloader, desc='iteration_{}'.format(epoch))):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        # print(b_labels)
        optimizer.zero_grad()
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs['loss']
        train_loss_set.append(loss.item())
        loss.backward()
        optimizer.step()
        scheduler.step()
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1
        
        if step % printing_step == 0 and not step == 0:
            elapsed = (datetime.utcnow() - t0).total_seconds()
            print('  Batch {:>5,}  of  {:>5,} in {}. Step loss={}'.format(step, len(train_dataloader), elapsed, loss.item()))

    print(f"Train loss: {tr_loss/nb_tr_steps}")

    # validation phase
    # put the model in evaluation mode to  evaluate loss on the validation set
    model.eval()

    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    for batch in validation_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        with torch.no_grad():
            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)

        logits = logits['logits'].detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        tmp_eval_accuracy = accuracy_score(np.argmax(label_ids, axis=1), np.argmax(logits, axis=1))

        eval_accuracy += tmp_eval_accuracy
        nb_eval_steps += 1

    print(f"Validation accuracy: {eval_accuracy/nb_eval_steps}")

    # save the model
    model_path = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model_v3'
    model.save_pretrained(model_path)
    tokenizer.save_pretrained(model_path)

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
plt.title("Training loss")
plt.xlabel("Batch")
plt.ylabel("loss")
plt.plot(train_loss_set)
plt.show()



"""## save the model"""

# model_path = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model'
# model.save_pretrained(model_path)
# tokenizer.save_pretrained(model_path)

"""## load the model"""

import torch
from transformers import BertTokenizer, BertForSequenceClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

number_labels = 5

# model_path = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model'
# model_path = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model_v2'
model_path = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model_v3'
model = BertForSequenceClassification.from_pretrained(model_path, num_labels=number_labels)
tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)
if device.type != 'cpu':
    model.cuda()

print(test_df.shape)
test_df.head()

from tqdm import tqdm
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler

sentences = test_df.reviewText.values
sentences = ["[CLS] " + sentence + " [SEP]" for sentence in sentences]
labels = test_df.overall.values

tokenized_text = [tokenizer.tokenize(sent) for sent in tqdm(sentences)]

MAX_LEN = 128

input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

attention_masks = []
for seq in input_ids:
    seq_mask = [float(i>0) for i in seq]
    attention_masks.append(seq_mask)


prediction_inputs = torch.tensor(input_ids)
prediction_masks = torch.tensor(attention_masks)
prediction_labels = torch.tensor(labels)

batch_size = 32

prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)
prediction_sampler = RandomSampler(prediction_data)
prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)

def prediction(prediction_dataloader, num_test_sents):
    print('Predicting labels for {:,} test sentences...'.format(num_test_sents))

    # Put model in evaluation mode
    model.eval()

    # Tracking variables 
    predictions , true_labels = [], []

    # Predict 
    for batch in tqdm(prediction_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        
        # Telling the model not to compute or store gradients, saving memory and 
        # speeding up prediction
        with torch.no_grad():
            # Forward pass, calculate logit predictions
            outputs = model(b_input_ids, token_type_ids=None, 
                            attention_mask=b_input_mask)

        logits = outputs[0]

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        
        # Store predictions and true labels
        predictions.extend(logits)
        true_labels.extend(label_ids)

    return predictions, true_labels

# prediction_dataloader = create_prediction_dataloader(sentences, labels)
predictions, true_labels = prediction(prediction_dataloader, len(prediction_inputs))

import numpy as np

predictions = np.vstack(predictions)
true_labels = np.vstack(true_labels)

test_arrays = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model/test_data.npz'
np.savez(test_arrays, predictions=predictions, true_labels=true_labels)

# data = np.load('mat.npz')
# print data['name1']
# print data['name2']

import numpy as np

test_arrays = '/content/drive/MyDrive/educative_natural_language_processing_for_professionals/models/transformer_model/test_data.npz'
test_arrays = np.load(test_arrays)
predictions = test_arrays['predictions']
true_labels = test_arrays['true_labels']

print(predictions.shape, true_labels.shape)

predictions

from sklearn.metrics import accuracy_score, matthews_corrcoef
from imblearn.metrics import classification_report_imbalanced

def print_classification_report(y_true, y_pred):
    y_pred_bool = np.argmax(y_pred, axis=1)
    if len(y_true.shape) == 2:
        y_true = y_true.reshape(-1) # flatten the true labels
        assert len(y_true.shape) == 1
    assert y_pred_bool.shape == y_true.shape, f'y_pred_bool.shape={y_pred_bool.shape}, y_true.shape={y_true.shape}'
    print('unique values for prediction', np.unique(y_pred_bool))
    print('unique values for true value', np.unique(y_true))
    print('accuracy:', accuracy_score(y_true, y_pred_bool + 1))
    print('matthews_corrcoef:', matthews_corrcoef(y_true, y_pred_bool + 1))
    print('classification_report:\n', classification_report_imbalanced(y_true, y_pred_bool + 1))


print_classification_report(true_labels, predictions)

from sklearn.metrics import accuracy_score, matthews_corrcoef
from imblearn.metrics import classification_report_imbalanced

def print_classification_report(y_true, y_pred):
    y_pred_bool = np.argmax(y_pred, axis=1)
    if len(y_true.shape) == 2:
        y_true = y_true.reshape(-1) # flatten the true labels
        assert len(y_true.shape) == 1
    assert y_pred_bool.shape == y_true.shape, f'y_pred_bool.shape={y_pred_bool.shape}, y_true.shape={y_true.shape}'
    print('unique values for prediction', np.unique(y_pred_bool))
    print('unique values for true value', np.unique(y_true))
    print('accuracy:', accuracy_score(y_true, y_pred_bool + 1))
    print('matthews_corrcoef:', matthews_corrcoef(y_true, y_pred_bool + 1))
    print('classification_report:\n', classification_report_imbalanced(y_true, y_pred_bool + 1))


print_classification_report(true_labels, predictions)

print('percentage improvement compared to the best model:', 100 * (0.564716129035763 - 0.33504969057289086) / 0.33504969057289086)

"""## Plot of model metrics till now"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

sns.set(rc={'figure.figsize':(11.7,8.27)})

# data
x = ["CV+NB",             "FF",               "CNN_trigram",      "CNN234GramModel",   "BERTSequenceModel"]
y = [0.33504969057289086, 0.2148927270393169, 0.3006712012400788, 0.29459743828288504, 0.564716129035763]

ax = sns.barplot(x=x, y=y)

"""### single prediction

single prediction can be done using the very useful TextClassificationPipeline from huggingface.
"""

from transformers import TextClassificationPipeline

device = 0 if torch.cuda.is_available() else -1 # for the pipeline

nlp_pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device) # device 0 for GPU

def printing_output(text, pipeline):
    print('input text: ', text)
    text = text_preprocessing(text)
    x_dict = pipeline(text)
    print('label: ', x_dict[0]['label'], 'confidence: ', x_dict[0]['score'])

printing_output('I liked the product quite a lot.', nlp_pipe)
printing_output('playing this was the worst time in my life.', nlp_pipe)

