# -*- coding: utf-8 -*-
"""string_processing_in_python

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CxtlV49Sm8lYTPelqHwe_5gKjfryJ9i9

## The `str` type.
"""

text = "quick brown fox jumps over the lazy dog."
print(type(text))

"""## Working with Unicode"""

text = "El Ni√±o".encode("utf-8")
print('original text: ', text)
print('text after decoding: ', text.decode("utf-8"))

"""### Size of different strings"""

import sys
string = 'hello'
print(sys.getsizeof(string))

# one byte encoding
print(sys.getsizeof(string+'!')-sys.getsizeof(string))

# 2-byte encoding
string2  = '‡¶Ö'
print(sys.getsizeof(string2+'‡¶Ö')-sys.getsizeof(string2))
print(sys.getsizeof(string2))

# 4-byte encoding
string3 = 'üêç'
print(sys.getsizeof(string3+'üíª')-sys.getsizeof(string3))
print(sys.getsizeof(string3))

"""## Common Python string methods

### Character lengths and word lengths
"""

string = 'natural language processing for professionals'
print('character length of the sentence:', len(string))

words = string.split()
print('word length of the sentence:', len(words))

"""### Character frequency"""

from collections import Counter

print(Counter('natural language processing for professionals'))

"""### Pattern Search"""

spam_string = "click on http://spam.com"

print('http' in spam_string)

import re

def url_present(text):
    pattern = 'http.*\.com'
    if re.search(pattern, spam_string):
        return True
    else:
        return False

spam_string = "click on http://spam.com"
print(url_present(spam_string))

spam_string = "update on ticket"
print(url_present(spam_string))

"""### Strip whitespace"""

string_with_whitespace = "    I am without whitespace. \n"
string_without_whitespace = "I am without whitespace."
print('Equality of the two strings: ',
  string_without_whitespace==string_with_whitespace)
print('Equality of the strings after performing strip on string_without_whitespace: ',
  string_with_whitespace.strip()==string_without_whitespace)

"""### Splitting Strings"""

document = "learning natural language processing"
print(document.split())

"""### Joining list elements in a contiguous string"""

tokens = ['learning', 'natural', 'language', 'processing']
print(" ".join(tokens))

"""### Case of the string"""

string = "EDUCATIVE"
print('unique object identification of `string`', id(string))

lower_case = string.lower()
print('sentence in lower case: ', lower_case)
print('unique object identification of `lower_case`', id(lower_case))

upper_case = lower_case.upper()
print('sentence in upper case: ', upper_case)
print('unique object identification of `upper_case`', id(upper_case))

print('is a match with the original text: ', upper_case == string)

"""## Pandas"""

import pandas as pd
s1 = pd.Series(
    ['string processing using vanila python',
     'string processing in pandas']
)
print(s1)
print()
print('converting all the strings to uppercase')
print(s1.str.upper())
print()
print('converting all the strings to lowercase')
print(s1.str.lower())
print()
print('split all the sentences to words')
print(s1.str.strip().str.split())

"""## Amazon reviews dataset"""

import urllib.request as req
from urllib.parse import urlparse
import os
import progressbar
import zipfile
import gzip
import shutil
import json
import pandas as pd
import re
import string

pbar = None


def show_progress(block_num, block_size, total_size):
    global pbar
    if pbar is None:
        pbar = progressbar.ProgressBar(maxval=total_size)
        pbar.start()

    downloaded = block_num * block_size
    if downloaded < total_size:
        pbar.update(downloaded)
    else:
        pbar.finish()
        pbar = None

def wget(url):
    a = urlparse(url)
    filename = os.path.basename(a.path)
    if not os.path.isfile(filename):
        req.urlretrieve(url, filename, show_progress)
        print(f'downloaded to {filename}')
    else:
        print(f'file {filename} has already been downloaded')
    return filename

def unzip(filename, directory_to_extract_to=os.getcwd()):
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall(directory_to_extract_to)
        print(f'extraction done {zip_ref.namelist()}')

def gunzip(gzfile, fout):
    with gzip.open(gzfile, 'rb') as f_in:
        with open(fout, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
    print(f'{gzfile} extracted to {fout}')


def parse(path):
    g = gzip.open(path, 'rb')
    for l in g:
        yield json.loads(l)

def getDF(path):
    df = {}
    i = 0
    for d in parse(path):
        df[i] = d
        i += 1
    return pd.DataFrame.from_dict(df, orient='index')


# map punctuation to space
translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) 

def text_preprocessing(df):
    """
    Preprocess the text for better understanding
    
    """
    # trim the whitespace at the edges of the string
    df['reviewText'] = df['reviewText'].str.strip()

    # lowercase the text in the string
    df['reviewText'] = df['reviewText'].str.lower()

    # replace new line with a .

    df['reviewText'] = df['reviewText'].replace('\n', '.')

    # # remove the punctualtion in the string.
    # df['reviewText'] = df['reviewText'].str.replace('.', ' [EOS] ')
    # df['reviewText'] = df['reviewText'].apply(lambda text: text.translate(translator))
    # df['reviewText'] = df['reviewText'].str.replace('[EOS]', ' . ')

    return df


filename = wget("https://nlp.stanford.edu/data/glove.6B.zip")
unzip(filename)
Video_Games_5 = wget('http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz')
df = getDF(Video_Games_5)
df = df[['reviewText', 'overall']]
print(df.shape)

import string

# map punctuation to space
translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) 

def text_preprocessing(df):
    """
    Preprocess the text for better understanding
    
    """
    # trim the whitespace at the edges of the string
    df['reviewText'] = df['reviewText'].str.strip()

    # lowercase the text in the string
    df['reviewText'] = df['reviewText'].str.lower()

    # remove the punctualtion in the string.
    df['reviewText'] = df['reviewText'].apply(lambda text: text.translate(translator))

    return df

df = df.dropna()
df = df.drop_duplicates()
df = text_preprocessing(df)

df.head()

